from es_client import es
now_ts = int(datetime.utcnow().timestamp() * 1000)
from datetime import datetime

INDEX_NAME = "rag_documents"

def search_with_hybrid_score(query: str, embedding: list[float], now_ts: int, top_k: int = 10):
    body = {
        "size": top_k,
        "query": {
            "script_score": {
                "query": {
                    "match": {
                        "content": query
                    }
                },
                "script": {
                    "source": """
                        double bm25 = _score;
                        double norm_bm25 = Math.log(1 + bm25) / Math.log(11);

                        double cosine = cosineSimilarity(params.query_vector, 'embedding_vector');
                        double norm_cosine = (cosine + 1.0) / 2.0;

                        double hours = (params.now - doc['publish_time'].value.toInstant().toEpochMilli()) / 3600000.0;
                        double time_boost = 1.0 / (1.0 + hours / 24.0);

                        return 0.5 * norm_bm25 + 0.4 * norm_cosine + 0.1 * time_boost;
                    """,
                    "params": {
                        "query_vector": embedding,
                        "now": now_ts
                    }
                }
            }
        }
    }

    response = es.search(index=INDEX_NAME, body=body)
    hits = response["hits"]["hits"]
    return [
        {
            "id": hit["_id"],
            "score": hit["_score"],
            "content": hit["_source"].get("content", ""),
            "publish_time": hit["_source"].get("publish_time", "")
        }
        for hit in hits
    ]




åœºæ™¯	Î± (BM25)	Î² (Cosine)	Î³ (Extra)
FAQåž‹é—®ç­”ï¼ˆå‘½ä¸­å…³é”®è¯ä¸ºä¸»ï¼‰	0.7	0.3	0.0
è¯­ä¹‰é—®ç­”ï¼ˆå¼€æ”¾æ€§ï¼‰	0.4	0.6	0.0
æ–°é—»ç±»ï¼ˆå…³æ³¨æ—¶é—´ï¼‰	0.5	0.4	0.1
å¤æ‚é—®ç­”ï¼ˆéœ€ LTRï¼‰	LTR æ›¿ä»£ä¸Šå¼ï¼Œæˆ–åŠ å…¥ç‰¹å¾åˆ†





Enterprise-Grade Hybrid Scoring Strategy for RAG

This strategy is designed to combine keyword relevance (BM25), semantic similarity (cosine similarity), and optional business-specific features into a unified ranking score for use in Retrieval-Augmented Generation (RAG) systems.


FinalScore = Î± * NormBM25 + Î² * NormCosine + Î³ * ExtraScore
Î±, Î², Î³ are tunable weights.

âœ… 1. Unified Scoring Formula
The final document relevance score is computed as a weighted combination:

FinalScore = Î± * NormBM25 + Î² * NormCosine + Î³ * ExtraScore
Î±, Î², Î³ are tunable weights.

NormBM25 is the normalized BM25 relevance score.

NormCosine is the normalized cosine similarity score.

ExtraScore is an optional field (e.g., time-based decay, popularity score, user engagement).

âœ… 2. Score Normalization Techniques
ðŸ”¹ BM25 Normalization
BM25 scores (_score in Elasticsearch) do not have a fixed range, so normalization is required.

 Log-Based Normalization (recommended for ES)

norm_bm25 = Math.log(1 + _score) / Math.log(1 + bm25_max)
You can assume bm25_max = 10.0 as a starting point.

ðŸ”¹ Cosine Similarity Normalization
Elasticsearchâ€™s cosine similarity returns values between -1 and 1, so we shift and scale it:


double cosine = cosineSimilarity(params.query_vector, 'embedding_vector');
double norm_cosine = (cosine + 1.0) / 2.0;  // Result: [0, 1]

ðŸ”¹ Extra Feature Score (Optional)
For example, time decay scoring:


double age_ms = params.now - doc['publish_time'].value.toInstant().toEpochMilli();
double hours = age_ms / 3600000.0;
double time_score = 1.0 / (1.0 + hours / 24.0);  // Prefer recent documents
âœ… 3. Elasticsearch script_score Query (Full Example)

{
  "size": 10,
  "query": {
    "script_score": {
      "query": {
        "match": {
          "content": "{{ your_query }}"
        }
      },
      "script": {
        "source": """
          double bm25 = _score;
          double norm_bm25 = Math.log(1 + bm25) / Math.log(11);

          double cosine = cosineSimilarity(params.query_vector, 'embedding_vector');
          double norm_cosine = (cosine + 1.0) / 2.0;

          double hours = (params.now - doc['publish_time'].value.toInstant().toEpochMilli()) / 3600000.0;
          double time_boost = 1.0 / (1.0 + hours / 24.0);

          return 0.5 * norm_bm25 + 0.4 * norm_cosine + 0.1 * time_boost;
        """,
        "params": {
          "query_vector": [0.1, 0.2, 0.3, ...],
          "now": 1690000000000
        }
      }
    }
  }
}
âœ… 4. Recommended Weight Settings by Scenario
Use Case	Î± (BM25)	Î² (Cosine)	Î³ (Time/Extra)
FAQ-style retrieval	0.7	0.3	0.0
Open-ended semantic query	0.4	0.6	0.0
News/article ranking	0.5	0.4	0.1
Financial/legal QA	0.5	0.5	0.0â€“0.1

âœ… 5. Optional Enhancements
Cross-Encoder Re-ranking: Use a BERT model to re-rank Top-N retrieved passages with full semantic matching.

Learning-to-Rank (LTR): Use XGBoost, LightGBM, or neural models trained on feedback (clicks, ratings).

Dynamic weight control: Tune Î±/Î²/Î³ based on query type or user profile.

Vector quantization or ANN indexing for faster retrieval at scale.




| Use Case                  | Î± (BM25) | Î² (Cosine) | Î³ (Time/Extra) |
| ------------------------- | -------- | ---------- | -------------- |
| FAQ-style retrieval       | 0.7      | 0.3        | 0.0            |
| Open-ended semantic query | 0.4      | 0.6        | 0.0            |
| News/article ranking      | 0.5      | 0.4        | 0.1            |
| Financial/legal QA        | 0.5      | 0.5        | 0.0â€“0.1        |





def split_text_with_metadata(text: str, source: str, page: int = None):
    chunks = splitter.split_text(text)
    return [
        {
            "content": chunk,
            "tokens": tiktoken_len(chunk),
            "metadata": {
                "source": source,
                "page": page
            }
        }
        for chunk in chunks
    ]




from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import GPT2TokenizerFast

# Tokenizer compatible with Google text-embedding-005
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

def tiktoken_len(text: str) -> int:
    return len(tokenizer.encode(text))

splitter = RecursiveCharacterTextSplitter(
    chunk_size=350,             # target ~350 tokens
    chunk_overlap=50,           # allow 50 token overlap
    length_function=tiktoken_len,
    separators=["\n\n", "\n", ".", " ", ""],  # hierarchy of splitting
)

def split_text(text: str):
    return splitter.split_text(text)

